\documentclass{article}
\usepackage{fixltx2e}
\usepackage{amssymb,amsmath}

\begin{document}

\title{PCA summary}
\author{L Oswald}
\date{}

\maketitle



\section{What is PCA?}

Principle component analysis (PCA) takes a data set described by basis \textbf{X} and re-expresses it in terms of a new basis \textbf{Y}, which is a linear combination of the original basis such that meaningful information can be extracted from the data. . .
The goal is to take a set of free-form mass distribution models of a gravitational lens and from these generate a parameterised functional form for the mass distribution which best fits the free-form models.
The free-form models are generated by SpaghettiLens: the program takes user input of positions of maxima, minima and saddle points and finds an ensemble of 200 mass distributions which exactly reproduce these positions. As there are an infinite number of mass distributions which would reproduce these positions, the ensemble generated also obey some priors, as described in . . . paper . . .
In PCA the data set is a mxn matrix \textbf{X} (m rows, n columns) where each column is a set of measurements from 1 trial and each row gives all measurements of a particular type. In this case, \textbf{X} is a 625x200 matrix: each mass distribution is made up of a 25x25 grid of mass pixels, giving 625 data points per distribution and there are 200 distributions.
To convert from \textbf{X} to \textbf{Y} we need to find the matrix \textbf{M} such that \textbf{MX = Y}. Rows \textbf{m\textsubscript{i}} of \textbf{M} are the principal components of \textbf{X}, that is to say they are a set of basis vectors for representing the columns (individual mass distributions) of \textbf{X}.
N.B. measurements should have zero means (why?) so in method the mean is subtracted from each data point before eigenvectors are found.



The principal components \textbf{m\textsubscript{i}} of \textbf{X} are a set of orthonormal basis vectors for \textbf{X} in the directions in which the variance is maximised, and they are the eigenvectors of the moment of inertia tensor of \textbf{X} (also known as the covariance matrix). Finding the mean and the inertia tensor of \textbf{X} provides all the information necessary for optimising a parameterised model of the data.
The inertia tensor is found by forming a matrix of the outer products of pairs of values in \textbf{X} and scaling them by the number of free form models:
\begin{equation}
M_{ij} = \frac{1}{n}\sum\nolimits \Delta x_i \Delta x_j
\end{equation}
It is a square symmetric mxm matrix (625 x 625) where the diagonal terms give the variance of each freeform distribution model ("assumption is that dynamics of interest exist along directions with largest variance and presumably highest signal to noise ratio‚Äù) and the off-diagonal terms are the covariance between measurement types, where the variance and covariance are
\begin{equation*}
\sigma^2 = \frac{1}{n}\sum\nolimits x_i x_j
\end{equation*}
where \begin{equation*} i = j \end{equation*} for the variance and \begin{equation*} i \neq j \end{equation*} for the covariance.


******************

Big values on diagonal = interesting structure, others = noise, big values off-diagonal = high redundancy i.e. high correlation between measurements at this point.

*****************


\section{Using PCA}
Once eigenvectors of MoI tensor have been found, they can be used to optimise a chosen parameterised model. There are two methods of doing this.

Chisquared: asks how far from mean along principle axes? Larger eigenvalue means the more it's allowed to go along that axis. 
Alternative method says that principle axes (i.e. axes with largest eigenvalues) are most important and the others are just noise and need to be minimised. Hence take parammodel - mean - SIGMA(parammodel dot eigenvector)*eigenvector (for biggest 5 or so eigenvectors) and find parameters to minimise this i.e. find params to fit the param model to the principle axes of the free-form model as closely as possible.

Found that chisquared is often useless (show pics of how rubbish maybe) and often depends strongly on the choice of initial parameters input, but other method is reasonable: fit seems to depend only on the choice of parameterised model input suggesting that the method is successfully optimising the fit even if the best fit for a particular model turns out not to be very good.

\section{Types of parameterised form tested}
Fitting eqns (-ve power law or something?) (depends on the equation but can be very good): parameters not necessarily specific to features of the lens, 
turning grav ptl into mass distrib (depends on lens: works well for more elliptical lens but less well for the more twisted lens as expected given that it's from a grav ptl model for an isothermal ellipsoid): parameters are Einstein radius, ellipticity and position angle of ellipticity

\section{Plotty stuff}
Can plot param form against mean of ensemble or against points from ensemble that fit the parameterised form most closely.


\section{bla}
Good summary of science so far and methods used, now just need to put it into proper sciency language and put nice equations in etc. Need to think more about reasons why chi squared is so rubbish

***********


s=0 but why? s is the scale radius but what does that mean?
"in limit of singular (s=0) and spherical (q=1) model, b = Einst radius"
but we are assuming b=reinst and s=0 and not q=1
    all sims have density go to infinity at origin so s=0 (makes eqns simpler) and this is a reasonable approximation to make for real galaxies (often black hole at centre)
    

add in shear factor... external sources of mass



\end{document}