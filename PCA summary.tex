\documentclass{article}

\begin{document}

\title{PCA summary}
\author{L Oswald}
\date{}

\maketitle



\section{What is PCA?}
Data: mxn matrix \textbf{X} (m rows, n columns) where column = set of measurements from 1 trial, row = all measurements of a particular type.
In this case m = 625: each pixel in 25x25 grid of pixellated mass distribution model has a value for the density (mass per unit area) at that point so it is treated as having dimension 625, 
n = 200: there are 200 mass distrib models for each lens. 
(Source of data is SpaghettiLens: program takes user input of positions of maxima, minima and saddle points and GLASS finds an ensemble of ~200 mass distributions k(x,y) which exactly reproduce these positions (wrt some priors))

PCA asks "Is there another basis, \textbf{Y}, which is a linear combination of the original basis, that best re-expresses our data set?"

\textbf{PX = Y} where rows \textbf{p(i)} of \textbf{P} are a set of basis vectors for representing columns of \textbf{X} (i.e. model mass distributions for the lens), meaning they're the principal components of \textbf{X}.

N.B. measurements should have zero means (why?) so in method the mean is subtracted from each data point before eigenvectors are found.

Moment of inertia tensor (a.k.a. Covariance matrix) = 1/n(\textbf{XX}T i.e. outer products of pairs of values of the matrix (also written as SIGMADELTAKiDELTAKj - tidy up notation at the end) normalised by number of trials (free form models).
This is a square symmetric mxm matrix (625x625) where diagonal terms are "\emph{variance} of particular measurement types" i.e. variance of each freeform distrib model ("assumption is that dynamics of interest exist along directions with largest variance and presumably highest signal to noise ratio" go into this in more detail) and the off-diagonal terms are covariance between measurement types. Big values on diagonal = interesting structure, others = noise, big values off-diagonal = high redundancy i.e. high correlation between measurements at this point.

The principal components \textbf{p(i)} of \textbf{X} are a set of orthonormal basis vectors for \textbf{X} with directions in which the variance is maximised. These are the eigenvectors of the moment of inertia tensor (this method is known as eigenvector decomposition). Alternative method is singular value decomposition which I haven't read yet.

\section{Using PCA}
Once eigenvectors of MoI tensor have been found, they can be used to optimise a chosen parameterised model. There are two methods of doing this.

Chisquared: asks how far from mean along principle axes? Larger eigenvalue means the more it's allowed to go along that axis. 
Alternative method says that principle axes (i.e. axes with largest eigenvalues) are most important and the others are just noise and need to be minimised. Hence take parammodel - mean - SIGMA(parammodel dot eigenvector)*eigenvector (for biggest 5 or so eigenvectors) and find parameters to minimise this i.e. find params to fit the param model to the principle axes of the free-form model as closely as possible.

Found that chisquared is often useless (show pics of how rubbish maybe) and often depends strongly on the choice of initial parameters input, but other method is reasonable: fit seems to depend only on the choice of parameterised model input suggesting that the method is successfully optimising the fit even if the best fit for a particular model turns out not to be very good.

\section{Types of parameterised form tested}
Fitting eqns (-ve power law or something?) (depends on the equation but can be very good), 
turning grav ptl into mass distrib (depends on lens: works well for more elliptical lens but less well for the more twisted lens as expected given that it's from a grav ptl model for an isothermal ellipsoid)


\section{bla}
Good summary of science so far and methods used, now just need to put it into proper sciency language and put nice equations in etc. Need to think more about reasons why chi squared is so rubbish












\end{document}